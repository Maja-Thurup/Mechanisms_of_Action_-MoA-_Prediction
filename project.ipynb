{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerated Linear Algebra enabled\n"
     ]
    }
   ],
   "source": [
    "MIXED_PRECISION = False\n",
    "XLA_ACCELERATE = True\n",
    "\n",
    "if MIXED_PRECISION:\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
    "    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy)\n",
    "    print('Mixed precision enabled')\n",
    "\n",
    "if XLA_ACCELERATE:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print('Accelerated Linear Algebra enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('train_features.csv')\n",
    "train_targets = pd.read_csv('train_targets_scored.csv')\n",
    "test_features = pd.read_csv('test_features.csv')\n",
    "\n",
    "ss = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 1, 'ctl_vehicle': 0})\n",
    "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_features = preprocess(train_features)\n",
    "test_features = preprocess(test_features)\n",
    "\n",
    "train = train_features.copy()\n",
    "test = test_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attr = pd.DataFrame()\n",
    "test_attr = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "GENES = [col for col in train.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train.columns if col.startswith('c-')]\n",
    "for stats in tqdm.tqdm(['sum', 'mean', 'std', 'kurt', 'skew']):\n",
    "    train_attr['g_'+stats] = getattr(train[GENES], stats)(axis=1)\n",
    "    train_attr['c_'+stats] = getattr(train[CELLS], stats)(axis=1)\n",
    "    train_attr['gc_'+stats] = getattr(train[GENES+CELLS], stats)(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  8.57it/s]\n"
     ]
    }
   ],
   "source": [
    "GENES = [col for col in test.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in test.columns if col.startswith('c-')]\n",
    "for stats in tqdm.tqdm(['sum', 'mean', 'std', 'kurt', 'skew']):\n",
    "    test_attr['g_'+stats] = getattr(test[GENES], stats)(axis=1)\n",
    "    test_attr['c_'+stats] = getattr(test[CELLS], stats)(axis=1)\n",
    "    test_attr['gc_'+stats] = getattr(test[GENES+CELLS], stats)(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genes = [s for s in train_features.columns if \"g-\" in s]\n",
    "train_cellvia = [s for s in train_features.columns if \"c-\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "cell_comp = 50\n",
    "gene_comp = 600\n",
    "\n",
    "pca = PCA(n_components=gene_comp)\n",
    "\n",
    "# pca.fit(train[:,2:])\n",
    "# train_pca = pca.transform(train[:,2:])\n",
    "# test_pca = pca.transform(test[:,2:])\n",
    "\n",
    "pca.fit(train.loc[:, train_genes])\n",
    "train_gene = pca.transform(train.loc[:, train_genes])\n",
    "test_gene = pca.transform(test.loc[:, train_genes])\n",
    "\n",
    "train_gene = pd.DataFrame(train_gene, columns=[f'pca_g-{i}' for i in range(gene_comp)])\n",
    "test_gene = pd.DataFrame(test_gene, columns=[f'pca_g-{i}' for i in range(gene_comp)])\n",
    "\n",
    "pca = PCA(n_components=cell_comp)\n",
    "pca.fit(train.loc[:, train_cellvia])\n",
    "train_cell = pca.transform(train.loc[:, train_cellvia])\n",
    "test_cell = pca.transform(test.loc[:, train_cellvia])\n",
    "\n",
    "train_cell = pd.DataFrame(train_cell, columns=[f'pca_c-{i}' for i in range(cell_comp)])\n",
    "test_cell = pd.DataFrame(test_cell, columns=[f'pca_c-{i}' for i in range(cell_comp)])\n",
    "\n",
    "train_cat = train.iloc[:,:4]\n",
    "test_cat = test.iloc[:,:4]\n",
    "\n",
    "# train = np.concatenate([train_pca, train_cat], axis=1)\n",
    "# test = np.concatenate([test_pca, test_cat], axis=1)\n",
    "\n",
    "train_pca = pd.concat([train_gene, train_cell], axis=1)\n",
    "test_pca = pd.concat([test_gene, test_cell], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train.iloc[:,4:], train_pca], axis=1)\n",
    "test = pd.concat([test.iloc[:,4:], test_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "train = pd.DataFrame(train)\n",
    "test = pd.DataFrame(test)\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(0.8)  #<-- Update\n",
    "data = train.append(test)\n",
    "data_transformed = var_thresh.fit_transform(data)\n",
    "\n",
    "train_features_transformed = data_transformed[ : train.shape[0]]\n",
    "test_features_transformed = data_transformed[-test.shape[0] : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_cat, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "test = pd.concat([test_cat, pd.DataFrame(test_features_transformed)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n",
    "    \n",
    "    features_g = list(train.columns[4:776])\n",
    "    features_c = list(train.columns[776:876])\n",
    "    \n",
    "    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n",
    "        train_ = train[features].copy()\n",
    "        test_ = test[features].copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n",
    "        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n",
    "        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n",
    "        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "        return train, test\n",
    "    \n",
    "    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n",
    "    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n",
    "    return train, test\n",
    "\n",
    "train , test = fe_cluster(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, train_attr], axis=1)\n",
    "test = pd.concat([test, test_attr], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(train_targets, on='sig_id')\n",
    "train = train[train['cp_type']!=0].reset_index(drop=True)\n",
    "\n",
    "cp_type0 = test[test['cp_type']==0].reset_index(drop=True)\n",
    "test = test[test['cp_type']!=0].reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_targets = train[train_targets.columns]\n",
    "\n",
    "target_cols = train_targets.drop('sig_id', axis=1).columns.values.tolist()\n",
    "\n",
    "train_cols = [ c for c in train if c not in target_cols]\n",
    "\n",
    "train = train[train_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['sig_id'], train[\"sig_id\"], train_targets['sig_id']\n",
    "\n",
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data\n",
    "\n",
    "train = process_data(train)\n",
    "test = process_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats = [   0,    1,    2,    3,    4,    6,    7,    8,    9,   10,   11,\n",
    "         12,   14,   15,   16,   17,   18,   19,   20,   21,   22,   25,\n",
    "         26,   27,   28,   29,   30,   31,   32,   33,   34,   35,   36,\n",
    "         37,   39,   40,   42,   43,   44,   45,   46,   47,   48,   49,\n",
    "         50,   51,   52,   53,   54,   55,   56,   57,   58,   59,   60,\n",
    "         61,   62,   64,   65,   68,   69,   71,   72,   73,   75,   76,\n",
    "         77,   78,   79,   81,   82,   83,   84,   85,   86,   87,   88,\n",
    "         89,   90,   91,   92,   94,   95,   96,   97,   98,   99,  101,\n",
    "        102,  103,  104,  105,  106,  107,  108,  109,  110,  111,  114,\n",
    "        115,  116,  117,  119,  120,  121,  122,  123,  124,  125,  126,\n",
    "        127,  128,  129,  130,  131,  132,  133,  134,  135,  136,  137,\n",
    "        138,  139,  140,  141,  142,  143,  144,  145,  146,  147,  148,\n",
    "        149,  150,  151,  152,  153,  154,  156,  157,  158,  159,  160,\n",
    "        162,  163,  164,  165,  166,  167,  168,  169,  170,  171,  172,\n",
    "        173,  175,  176,  177,  178,  179,  181,  183,  184,  185,  186,\n",
    "        187,  189,  190,  191,  192,  193,  195,  196,  197,  198,  199,\n",
    "        200,  201,  202,  203,  205,  206,  207,  209,  210,  211,  214,\n",
    "        216,  218,  220,  221,  223,  225,  227,  228,  229,  230,  231,\n",
    "        232,  233,  234,  235,  236,  237,  238,  239,  240,  241,  242,\n",
    "        244,  245,  246,  247,  248,  249,  250,  253,  254,  255,  256,\n",
    "        257,  258,  259,  260,  262,  263,  265,  266,  267,  268,  269,\n",
    "        270,  272,  273,  275,  276,  277,  278,  279,  282,  283,  284,\n",
    "        285,  286,  287,  288,  289,  290,  291,  292,  293,  294,  296,\n",
    "        297,  298,  299,  300,  301,  302,  303,  304,  305,  306,  307,\n",
    "        308,  309,  311,  312,  313,  314,  315,  316,  317,  318,  319,\n",
    "        320,  321,  323,  325,  326,  327,  328,  329,  330,  332,  333,\n",
    "        336,  337,  338,  339,  340,  341,  342,  343,  344,  345,  346,\n",
    "        348,  349,  350,  351,  352,  353,  354,  356,  357,  358,  359,\n",
    "        360,  361,  362,  363,  365,  368,  370,  371,  372,  373,  375,\n",
    "        376,  377,  378,  379,  380,  381,  382,  383,  384,  385,  387,\n",
    "        388,  390,  392,  393,  394,  397,  398,  400,  401,  402,  403,\n",
    "        404,  405,  406,  407,  408,  410,  411,  413,  414,  415,  416,\n",
    "        417,  418,  419,  420,  421,  422,  423,  424,  425,  426,  427,\n",
    "        428,  429,  432,  433,  435,  436,  437,  438,  439,  440,  441,\n",
    "        442,  443,  444,  447,  448,  449,  451,  452,  453,  454,  455,\n",
    "        456,  458,  460,  461,  462,  463,  464,  465,  468,  469,  471,\n",
    "        472,  473,  474,  475,  476,  478,  479,  480,  481,  483,  485,\n",
    "        486,  487,  488,  489,  490,  491,  494,  495,  496,  497,  498,\n",
    "        502,  503,  504,  505,  506,  507,  508,  509,  510,  511,  512,\n",
    "        513,  514,  515,  518,  519,  520,  521,  522,  524,  525,  528,\n",
    "        529,  530,  531,  532,  538,  539,  540,  541,  542,  544,  545,\n",
    "        546,  548,  549,  551,  552,  553,  554,  555,  558,  559,  560,\n",
    "        561,  563,  565,  566,  567,  568,  569,  570,  571,  572,  573,\n",
    "        574,  575,  576,  577,  578,  579,  580,  581,  584,  586,  588,\n",
    "        590,  591,  592,  595,  596,  597,  598,  599,  600,  601,  602,\n",
    "        603,  604,  605,  606,  607,  608,  610,  611,  612,  613,  615,\n",
    "        616,  617,  619,  620,  621,  622,  623,  625,  626,  627,  628,\n",
    "        629,  632,  633,  634,  636,  637,  638,  639,  640,  643,  645,\n",
    "        646,  647,  649,  650,  651,  652,  656,  657,  659,  660,  661,\n",
    "        662,  663,  664,  665,  666,  667,  668,  669,  670,  671,  673,\n",
    "        674,  675,  677,  679,  680,  681,  683,  684,  685,  686,  688,\n",
    "        689,  690,  691,  692,  693,  694,  695,  696,  697,  698,  700,\n",
    "        701,  702,  703,  704,  705,  706,  707,  708,  709,  710,  711,\n",
    "        712,  713,  714,  715,  716,  717,  718,  719,  720,  721,  722,\n",
    "        723,  724,  725,  726,  727,  728,  729,  730,  731,  732,  733,\n",
    "        734,  735,  736,  737,  738,  739,  740,  741,  742,  744,  745,\n",
    "        746,  747,  750,  751,  752,  753,  754,  755,  756,  757,  758,\n",
    "        759,  760,  761,  762,  763,  764,  765,  767,  768,  769,  770,\n",
    "        771,  772,  773,  774,  775,  776,  777,  778,  779,  780,  781,\n",
    "        782,  783,  784,  785,  786,  787,  788,  789,  790,  791,  792,\n",
    "        793,  794,  795,  796,  797,  798,  799,  800,  801,  802,  803,\n",
    "        804,  805,  806,  807,  808,  809,  810,  811,  812,  813,  814,\n",
    "        815,  816,  817,  818,  819,  820,  821,  822,  823,  824,  825,\n",
    "        826,  827,  828,  829,  830,  831,  832,  833,  834,  835,  836,\n",
    "        837,  839,  840,  841,  842,  843,  844,  845,  846,  847,  848,\n",
    "        849,  850,  851,  852,  853,  854,  855,  856,  857,  858,  859,\n",
    "        860,  861,  862,  863,  864,  865,  866,  867,  868,  870,  872,\n",
    "        873,  874,  875,  876,  878,  879,  880,  881,  882,  883,  885,\n",
    "        887,  888,  889,  890,  892,  893,  894,  895,  896,  898,  899,\n",
    "        900,  901,  902,  903,  905,  906,  907,  908,  910,  913,  914,\n",
    "        917,  918,  920,  921,  922,  923,  924,  926,  928,  929,  931,\n",
    "        932,  933,  934,  935,  936,  937,  938,  939,  940,  942,  943,\n",
    "        944,  945,  946,  948,  949,  950,  951,  952,  953,  954,  956,\n",
    "        957,  959,  960,  961,  962,  963,  964,  965,  966,  967,  969,\n",
    "        971,  972,  973,  976,  978,  980,  982,  983,  985,  986,  987,\n",
    "        988,  989,  990,  991,  992,  993,  994,  995,  996,  997,  998,\n",
    "        999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1008, 1009, 1010,\n",
    "       1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021,\n",
    "       1022, 1026, 1027, 1029, 1030, 1031, 1034, 1037, 1038, 1040, 1045,\n",
    "       1047, 1048, 1051, 1052, 1053, 1055, 1056, 1057, 1062, 1063, 1064,\n",
    "       1066, 1068, 1069, 1070, 1071, 1072, 1073, 1075, 1077, 1079, 1080,\n",
    "       1081]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.values[:, top_feats]\n",
    "test = test.values[:, top_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train)\n",
    "test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892\n",
      "206\n"
     ]
    }
   ],
   "source": [
    "col_num_feat = train.shape[1]\n",
    "col_num_tar = len(train_targets.columns)\n",
    "\n",
    "print(col_num_feat)\n",
    "print(col_num_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  acat_inhibitor  \\\n",
       "0                                0                       0               0   \n",
       "1                                0                       0               0   \n",
       "2                                0                       0               0   \n",
       "3                                0                       0               0   \n",
       "4                                0                       0               0   \n",
       "...                            ...                     ...             ...   \n",
       "21943                            0                       0               0   \n",
       "21944                            0                       0               0   \n",
       "21945                            0                       0               0   \n",
       "21946                            0                       0               0   \n",
       "21947                            0                       0               0   \n",
       "\n",
       "       acetylcholine_receptor_agonist  acetylcholine_receptor_antagonist  \\\n",
       "0                                   0                                  0   \n",
       "1                                   0                                  0   \n",
       "2                                   0                                  0   \n",
       "3                                   0                                  0   \n",
       "4                                   0                                  0   \n",
       "...                               ...                                ...   \n",
       "21943                               0                                  0   \n",
       "21944                               0                                  0   \n",
       "21945                               0                                  0   \n",
       "21946                               0                                  0   \n",
       "21947                               0                                  0   \n",
       "\n",
       "       acetylcholinesterase_inhibitor  adenosine_receptor_agonist  \\\n",
       "0                                   0                           0   \n",
       "1                                   0                           0   \n",
       "2                                   0                           0   \n",
       "3                                   0                           0   \n",
       "4                                   0                           0   \n",
       "...                               ...                         ...   \n",
       "21943                               0                           0   \n",
       "21944                               0                           0   \n",
       "21945                               0                           0   \n",
       "21946                               0                           0   \n",
       "21947                               0                           0   \n",
       "\n",
       "       adenosine_receptor_antagonist  adenylyl_cyclase_activator  \\\n",
       "0                                  0                           0   \n",
       "1                                  0                           0   \n",
       "2                                  0                           0   \n",
       "3                                  0                           0   \n",
       "4                                  0                           0   \n",
       "...                              ...                         ...   \n",
       "21943                              0                           0   \n",
       "21944                              0                           0   \n",
       "21945                              0                           0   \n",
       "21946                              0                           0   \n",
       "21947                              0                           0   \n",
       "\n",
       "       adrenergic_receptor_agonist  ...  \\\n",
       "0                                0  ...   \n",
       "1                                0  ...   \n",
       "2                                0  ...   \n",
       "3                                0  ...   \n",
       "4                                0  ...   \n",
       "...                            ...  ...   \n",
       "21943                            0  ...   \n",
       "21944                            0  ...   \n",
       "21945                            0  ...   \n",
       "21946                            0  ...   \n",
       "21947                            0  ...   \n",
       "\n",
       "       tropomyosin_receptor_kinase_inhibitor  trpv_agonist  trpv_antagonist  \\\n",
       "0                                          0             0                0   \n",
       "1                                          0             0                0   \n",
       "2                                          0             0                0   \n",
       "3                                          0             0                0   \n",
       "4                                          0             0                0   \n",
       "...                                      ...           ...              ...   \n",
       "21943                                      0             0                0   \n",
       "21944                                      0             0                0   \n",
       "21945                                      0             0                0   \n",
       "21946                                      0             0                0   \n",
       "21947                                      0             0                0   \n",
       "\n",
       "       tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "0                      0                          0   \n",
       "1                      0                          0   \n",
       "2                      0                          0   \n",
       "3                      0                          0   \n",
       "4                      0                          0   \n",
       "...                  ...                        ...   \n",
       "21943                  0                          0   \n",
       "21944                  0                          0   \n",
       "21945                  0                          0   \n",
       "21946                  0                          0   \n",
       "21947                  0                          0   \n",
       "\n",
       "       ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n",
       "0                                          0                0          0   \n",
       "1                                          0                0          0   \n",
       "2                                          0                0          0   \n",
       "3                                          0                0          0   \n",
       "4                                          0                0          0   \n",
       "...                                      ...              ...        ...   \n",
       "21943                                      0                0          0   \n",
       "21944                                      0                0          0   \n",
       "21945                                      0                0          0   \n",
       "21946                                      0                0          0   \n",
       "21947                                      0                0          0   \n",
       "\n",
       "       vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                               0              0  \n",
       "1                               0              0  \n",
       "2                               0              0  \n",
       "3                               0              0  \n",
       "4                               0              0  \n",
       "...                           ...            ...  \n",
       "21943                           0              0  \n",
       "21944                           0              0  \n",
       "21945                           0              0  \n",
       "21946                           0              0  \n",
       "21947                           0              0  \n",
       "\n",
       "[21948 rows x 206 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, AlphaDropout, Dense , Flatten ,Embedding, Input, LSTM, Bidirectional, BatchNormalization, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_columns, hidden_units, dropout_rate):\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape = (num_columns, ))\n",
    "#     x = tf.keras.layers.Dropout(dropout_rate)(inp)\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "#     x = tfa.layers.WeightNormalization(Dense(2048, activation = 'elu'))(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    for units in hidden_units:\n",
    "        \n",
    "        x = tfa.layers.WeightNormalization(Dense(units))(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "#         x = tf.keras.layers.BatchNormalization()(x)\n",
    "       \n",
    "#     x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    out = tfa.layers.WeightNormalization(Dense(206, activation = 'sigmoid'))(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(lr = 5e-4, weight_decay = 1e-5, clipvalue = 900), \n",
    "                  loss=BinaryCrossentropy(label_smoothing=5e-4))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ss.set_index('sig_id')\n",
    "ids = list(cp_type0['sig_id'])\n",
    "cp_type0 = ss[ss.index.isin(ids)].reset_index()\n",
    "ss = ss[~ss.index.isin(ids)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\13479\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:71: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0, Fold 0:\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Train on 19753 samples, validate on 2195 samples\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09893, saving model to model_0_0.hdf5\n",
      "19753/19753 - 4s - loss: 0.4692 - val_loss: 0.0989\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09893 to 0.03083, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0543 - val_loss: 0.0308\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03083 to 0.02370, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0278 - val_loss: 0.0237\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02370 to 0.02127, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0235 - val_loss: 0.0213\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02127 to 0.02045, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0218 - val_loss: 0.0205\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02045 to 0.01998, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0211 - val_loss: 0.0200\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01998 to 0.01945, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0204 - val_loss: 0.0194\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01945 to 0.01925, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0199 - val_loss: 0.0192\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01925 to 0.01897, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0196 - val_loss: 0.0190\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01897 to 0.01890, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0193 - val_loss: 0.0189\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01890 to 0.01864, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0191 - val_loss: 0.0186\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01864\n",
      "19753/19753 - 2s - loss: 0.0189 - val_loss: 0.0187\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01864 to 0.01833, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0187 - val_loss: 0.0183\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01833\n",
      "19753/19753 - 2s - loss: 0.0184 - val_loss: 0.0183\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01833 to 0.01832, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0183 - val_loss: 0.0183\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01832 to 0.01824, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0182 - val_loss: 0.0182\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01824 to 0.01803, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0177 - val_loss: 0.0180\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01803 to 0.01799, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0176 - val_loss: 0.0180\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01799\n",
      "19753/19753 - 2s - loss: 0.0176 - val_loss: 0.0180\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01799 to 0.01795, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0175 - val_loss: 0.0179\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01795\n",
      "19753/19753 - 2s - loss: 0.0174 - val_loss: 0.0179\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01795 to 0.01795, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0174 - val_loss: 0.0179\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01795 to 0.01794, saving model to model_0_0.hdf5\n",
      "19753/19753 - 2s - loss: 0.0174 - val_loss: 0.0179\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01794\n",
      "19753/19753 - 2s - loss: 0.0175 - val_loss: 0.0180\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01794\n",
      "19753/19753 - 2s - loss: 0.0174 - val_loss: 0.0180\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01794\n",
      "19753/19753 - 2s - loss: 0.0175 - val_loss: 0.0180\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01794\n",
      "Restoring model weights from the end of the best epoch.\n",
      "19753/19753 - 2s - loss: 0.0174 - val_loss: 0.0180\n",
      "Epoch 00027: early stopping\n",
      "Best Validation Loss:\t 0.01794494009133352\n",
      "Cost Time:\t 49.537208795547485\n",
      "--------------------------------------------------\n",
      "Seed 0, Fold 1:\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Train on 19753 samples, validate on 2195 samples\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11901, saving model to model_0_1.hdf5\n",
      "19753/19753 - 4s - loss: 0.4666 - val_loss: 0.1190\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11901 to 0.03040, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0543 - val_loss: 0.0304\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03040 to 0.02439, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0278 - val_loss: 0.0244\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02439 to 0.02129, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0234 - val_loss: 0.0213\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02129 to 0.02060, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0218 - val_loss: 0.0206\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02060 to 0.01973, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0208 - val_loss: 0.0197\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01973 to 0.01949, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0202 - val_loss: 0.0195\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01949\n",
      "19753/19753 - 2s - loss: 0.0200 - val_loss: 0.0196\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01949 to 0.01886, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0197 - val_loss: 0.0189\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01886 to 0.01872, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0193 - val_loss: 0.0187\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01872 to 0.01871, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0192 - val_loss: 0.0187\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01871 to 0.01856, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0190 - val_loss: 0.0186\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01856 to 0.01834, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0187 - val_loss: 0.0183\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01834\n",
      "19753/19753 - 2s - loss: 0.0184 - val_loss: 0.0185\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01834 to 0.01814, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0183 - val_loss: 0.0181\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01814\n",
      "19753/19753 - 2s - loss: 0.0183 - val_loss: 0.0182\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01814 to 0.01806, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0180 - val_loss: 0.0181\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01806 to 0.01802, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0178 - val_loss: 0.0180\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01802 to 0.01800, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0177 - val_loss: 0.0180\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01800\n",
      "19753/19753 - 2s - loss: 0.0175 - val_loss: 0.0180\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01800 to 0.01790, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0174 - val_loss: 0.0179\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01790 to 0.01789, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0171 - val_loss: 0.0179\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01789 to 0.01780, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0171 - val_loss: 0.0178\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01780\n",
      "19753/19753 - 2s - loss: 0.0170 - val_loss: 0.0179\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01780\n",
      "19753/19753 - 2s - loss: 0.0168 - val_loss: 0.0178\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01780\n",
      "19753/19753 - 2s - loss: 0.0167 - val_loss: 0.0179\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01780 to 0.01767, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0161 - val_loss: 0.0177\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01767 to 0.01765, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0160 - val_loss: 0.0176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01765 to 0.01764, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0159 - val_loss: 0.0176\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01764 to 0.01762, saving model to model_0_1.hdf5\n",
      "19753/19753 - 2s - loss: 0.0158 - val_loss: 0.0176\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01762\n",
      "19753/19753 - 2s - loss: 0.0158 - val_loss: 0.0176\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01762\n",
      "19753/19753 - 2s - loss: 0.0157 - val_loss: 0.0176\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01762\n",
      "19753/19753 - 2s - loss: 0.0158 - val_loss: 0.0176\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01762\n",
      "19753/19753 - 2s - loss: 0.0157 - val_loss: 0.0176\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01762\n",
      "19753/19753 - 2s - loss: 0.0158 - val_loss: 0.0176\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01762\n",
      "19753/19753 - 2s - loss: 0.0157 - val_loss: 0.0176\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01762\n",
      "Restoring model weights from the end of the best epoch.\n",
      "19753/19753 - 2s - loss: 0.0157 - val_loss: 0.0176\n",
      "Epoch 00037: early stopping\n",
      "Best Validation Loss:\t 0.017622065292947114\n",
      "Cost Time:\t 64.66263556480408\n",
      "--------------------------------------------------\n",
      "Seed 0, Fold 2:\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Train on 19753 samples, validate on 2195 samples\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09848, saving model to model_0_2.hdf5\n",
      "19753/19753 - 4s - loss: 0.4670 - val_loss: 0.0985\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09848 to 0.03376, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0546 - val_loss: 0.0338\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03376 to 0.02402, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0280 - val_loss: 0.0240\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02402 to 0.02199, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0235 - val_loss: 0.0220\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02199 to 0.02148, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0217 - val_loss: 0.0215\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02148 to 0.02027, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0210 - val_loss: 0.0203\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02027 to 0.01992, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0204 - val_loss: 0.0199\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01992 to 0.01947, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0199 - val_loss: 0.0195\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01947 to 0.01917, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0195 - val_loss: 0.0192\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01917 to 0.01896, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0193 - val_loss: 0.0190\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01896\n",
      "19753/19753 - 2s - loss: 0.0193 - val_loss: 0.0191\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01896 to 0.01886, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0190 - val_loss: 0.0189\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01886 to 0.01862, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0187 - val_loss: 0.0186\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01862\n",
      "19753/19753 - 2s - loss: 0.0184 - val_loss: 0.0187\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01862 to 0.01842, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0183 - val_loss: 0.0184\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01842 to 0.01839, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0181 - val_loss: 0.0184\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01839 to 0.01837, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0179 - val_loss: 0.0184\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01837\n",
      "19753/19753 - 2s - loss: 0.0178 - val_loss: 0.0184\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01837 to 0.01814, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0173 - val_loss: 0.0181\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01814 to 0.01811, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0172 - val_loss: 0.0181\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01811 to 0.01810, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0171 - val_loss: 0.0181\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01810 to 0.01807, saving model to model_0_2.hdf5\n",
      "19753/19753 - 2s - loss: 0.0171 - val_loss: 0.0181\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01807\n",
      "19753/19753 - 2s - loss: 0.0170 - val_loss: 0.0181\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01807\n",
      "19753/19753 - 2s - loss: 0.0170 - val_loss: 0.0181\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01807\n",
      "19753/19753 - 2s - loss: 0.0171 - val_loss: 0.0181\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01807\n",
      "19753/19753 - 2s - loss: 0.0170 - val_loss: 0.0181\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01807\n",
      "19753/19753 - 2s - loss: 0.0170 - val_loss: 0.0181\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01807\n",
      "19753/19753 - 2s - loss: 0.0170 - val_loss: 0.0181\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01807\n",
      "Restoring model weights from the end of the best epoch.\n",
      "19753/19753 - 2s - loss: 0.0171 - val_loss: 0.0181\n",
      "Epoch 00029: early stopping\n",
      "Best Validation Loss:\t 0.018068636367458146\n",
      "Cost Time:\t 51.84273076057434\n",
      "--------------------------------------------------\n",
      "Seed 0, Fold 3:\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Train on 19753 samples, validate on 2195 samples\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10349, saving model to model_0_3.hdf5\n",
      "19753/19753 - 4s - loss: 0.4709 - val_loss: 0.1035\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10349 to 0.03146, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0556 - val_loss: 0.0315\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03146 to 0.02362, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0278 - val_loss: 0.0236\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02362 to 0.02155, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0234 - val_loss: 0.0216\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02155 to 0.02054, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0218 - val_loss: 0.0205\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02054 to 0.02015, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0209 - val_loss: 0.0201\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02015 to 0.01956, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0204 - val_loss: 0.0196\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01956 to 0.01928, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0199 - val_loss: 0.0193\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01928 to 0.01926, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0198 - val_loss: 0.0193\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01926 to 0.01882, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0194 - val_loss: 0.0188\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01882 to 0.01878, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0193 - val_loss: 0.0188\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01878 to 0.01859, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0190 - val_loss: 0.0186\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01859 to 0.01842, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0187 - val_loss: 0.0184\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01842\n",
      "19753/19753 - 2s - loss: 0.0186 - val_loss: 0.0187\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01842 to 0.01831, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0184 - val_loss: 0.0183\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01831\n",
      "19753/19753 - 2s - loss: 0.0182 - val_loss: 0.0184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01831 to 0.01810, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0181 - val_loss: 0.0181\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01810\n",
      "19753/19753 - 2s - loss: 0.0180 - val_loss: 0.0182\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01810 to 0.01800, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0177 - val_loss: 0.0180\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01800\n",
      "19753/19753 - 2s - loss: 0.0175 - val_loss: 0.0180\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01800\n",
      "19753/19753 - 2s - loss: 0.0174 - val_loss: 0.0180\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01800 to 0.01788, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0174 - val_loss: 0.0179\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01788\n",
      "19753/19753 - 2s - loss: 0.0171 - val_loss: 0.0179\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01788\n",
      "19753/19753 - 2s - loss: 0.0170 - val_loss: 0.0180\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01788\n",
      "19753/19753 - 2s - loss: 0.0169 - val_loss: 0.0180\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01788 to 0.01774, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0164 - val_loss: 0.0177\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01774 to 0.01769, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0162 - val_loss: 0.0177\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01769 to 0.01766, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0162 - val_loss: 0.0177\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01766 to 0.01763, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0161 - val_loss: 0.0176\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01763 to 0.01763, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0160 - val_loss: 0.0176\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01763 to 0.01760, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0160 - val_loss: 0.0176\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01760\n",
      "19753/19753 - 2s - loss: 0.0159 - val_loss: 0.0176\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01760\n",
      "19753/19753 - 2s - loss: 0.0158 - val_loss: 0.0176\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01760 to 0.01760, saving model to model_0_3.hdf5\n",
      "19753/19753 - 2s - loss: 0.0158 - val_loss: 0.0176\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01760\n",
      "19753/19753 - 2s - loss: 0.0158 - val_loss: 0.0176\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01760\n",
      "19753/19753 - 2s - loss: 0.0158 - val_loss: 0.0176\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01760\n",
      "19753/19753 - 2s - loss: 0.0159 - val_loss: 0.0176\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01760\n",
      "19753/19753 - 2s - loss: 0.0158 - val_loss: 0.0176\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01760\n",
      "Restoring model weights from the end of the best epoch.\n",
      "19753/19753 - 2s - loss: 0.0158 - val_loss: 0.0176\n",
      "Epoch 00039: early stopping\n",
      "Best Validation Loss:\t 0.0176049212609038\n",
      "Cost Time:\t 68.54524874687195\n",
      "--------------------------------------------------\n",
      "Seed 0, Fold 4:\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Train on 19753 samples, validate on 2195 samples\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12017, saving model to model_0_4.hdf5\n",
      "19753/19753 - 4s - loss: 0.4685 - val_loss: 0.1202\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12017 to 0.03010, saving model to model_0_4.hdf5\n",
      "19753/19753 - 2s - loss: 0.0559 - val_loss: 0.0301\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03010 to 0.02393, saving model to model_0_4.hdf5\n",
      "19753/19753 - 2s - loss: 0.0278 - val_loss: 0.0239\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02393 to 0.02172, saving model to model_0_4.hdf5\n",
      "19753/19753 - 2s - loss: 0.0236 - val_loss: 0.0217\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02172 to 0.02081, saving model to model_0_4.hdf5\n",
      "19753/19753 - 2s - loss: 0.0218 - val_loss: 0.0208\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02081 to 0.02025, saving model to model_0_4.hdf5\n",
      "19753/19753 - 2s - loss: 0.0209 - val_loss: 0.0202\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02025 to 0.01976, saving model to model_0_4.hdf5\n",
      "19753/19753 - 2s - loss: 0.0205 - val_loss: 0.0198\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01976 to 0.01953, saving model to model_0_4.hdf5\n",
      "19753/19753 - 2s - loss: 0.0200 - val_loss: 0.0195\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01953 to 0.01937, saving model to model_0_4.hdf5\n",
      "19753/19753 - 2s - loss: 0.0197 - val_loss: 0.0194\n",
      "Epoch 10/100\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-4ece51e07bd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         history = model.fit(x_tr, y_tr, validation_data = (x_val, y_val), epochs = EPOCHS, \n\u001b[1;32m---> 43\u001b[1;33m                             batch_size = BATCH_SIZE, callbacks = [rlr, ckp, erl], verbose = 2)\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13479\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\13479\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13479\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    179\u001b[0m             batch_end=step * batch_size + current_batch_size)\n\u001b[0;32m    180\u001b[0m       \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m       \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13479\\anaconda3\\envs\\tf-gpu\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13479\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[1;34m(self, step, mode, size)\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_exhausted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m           self.callbacks._call_batch_hook(\n\u001b[1;32m--> 788\u001b[1;33m               mode, 'end', step, batch_logs)\n\u001b[0m\u001b[0;32m    789\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13479\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m     \u001b[0mdelta_t_median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[0;32m    244\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13479\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[1;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[0;32m   3493\u001b[0m     \"\"\"\n\u001b[0;32m   3494\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[1;32m-> 3495\u001b[1;33m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[0;32m   3496\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3497\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13479\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[1;34m(a, func, **kwargs)\u001b[0m\n\u001b[0;32m   3401\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3403\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3404\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13479\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_median\u001b[1;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[0;32m   3526\u001b[0m             \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3527\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3528\u001b[1;33m         \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpartition\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13479\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_STARTS = 1\n",
    "HIDDEN_UNITS = [4096, 512, 4096]\n",
    "# HIDDEN_UNITS = 400\n",
    "DROPOUT_RATE = 0.43912157\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "N_SPLITS = 10\n",
    "\n",
    "res = train_targets.copy()\n",
    "ss.loc[:, train_targets.columns] = 0\n",
    "res.loc[:, train_targets.columns] = 0\n",
    "\n",
    "for seed in range(N_STARTS):\n",
    "    \n",
    "    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits = N_SPLITS, \n",
    "                                                 random_state = seed, \n",
    "                                                 shuffle = True).split(train_targets, \n",
    "                                                                       train_targets)):\n",
    "        print(f'Seed {seed}, Fold {n}:')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with strategy.scope():\n",
    "        \n",
    "            model = create_model(col_num_feat, HIDDEN_UNITS, DROPOUT_RATE)\n",
    "            \n",
    "        rlr = ReduceLROnPlateau(monitor='val_loss', factor = 0.1, patience = 3, \n",
    "                                verbose = 0, epsilon = 1e-4, mode = 'min')\n",
    "        \n",
    "        ckp = ModelCheckpoint(f'model_{seed}_{n}.hdf5', monitor = 'val_loss', verbose = 1, \n",
    "                              save_best_only = True, save_weights_only = True, mode = 'min')\n",
    "        \n",
    "        erl = EarlyStopping(monitor = 'val_loss', \n",
    "                                min_delta = 1e-4, \n",
    "                                patience = 10, mode = 'min', \n",
    "                                baseline = None, \n",
    "                                restore_best_weights = True, \n",
    "                                verbose = 1)\n",
    "        \n",
    "        x_tr, x_val = train.values[tr], train.values[te]\n",
    "        y_tr, y_val = train_targets.astype(float).values[tr], train_targets.astype(float).values[te]\n",
    "        \n",
    "        history = model.fit(x_tr, y_tr, validation_data = (x_val, y_val), epochs = EPOCHS, \n",
    "                            batch_size = BATCH_SIZE, callbacks = [rlr, ckp, erl], verbose = 2)\n",
    "        \n",
    "        hist = pd.DataFrame(history.history)\n",
    "        \n",
    "        model.load_weights(f'model_{seed}_{n}.hdf5')\n",
    "\n",
    "        ss.loc[:, train_targets.columns] += model.predict(test)\n",
    "        res.loc[te, train_targets.columns] += model.predict(train.values[te])\n",
    "        print('Best Validation Loss:\\t', hist['val_loss'].min())\n",
    "        print('Cost Time:\\t', time.time() - start_time)\n",
    "        print('-' * 50)\n",
    "        \n",
    "        K.clear_session()\n",
    "        del model, history, hist\n",
    "        gc.collect()\n",
    "    \n",
    "ss.loc[:, train_targets.columns] /= ((n + 1) * N_STARTS)\n",
    "res.loc[:, train_targets.columns] /= N_STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for _target in train_targets.columns:\n",
    "    metrics.append(log_loss(train_targets.loc[:, _target], res.loc[:, _target]))\n",
    "    \n",
    "print(f'OOF Metric: {np.mean(metrics)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = res.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF Metric with postprocessing 0.0: 0.015759747867676487\n",
      "OOF Metric with postprocessing 0.005: 0.01575452165456358\n",
      "OOF Metric with postprocessing 0.01: 0.01575104883280062\n",
      "OOF Metric with postprocessing 0.015: 0.015748368467450594\n",
      "OOF Metric with postprocessing 0.02: 0.01574617859511822\n",
      "OOF Metric with postprocessing 0.025: 0.015744339118709753\n",
      "OOF Metric with postprocessing 0.03: 0.01574277134476866\n",
      "OOF Metric with postprocessing 0.035: 0.015741425697267483\n",
      "OOF Metric with postprocessing 0.04: 0.015740268463382848\n",
      "OOF Metric with postprocessing 0.045: 0.015739275435586328\n",
      "OOF Metric with postprocessing 0.05: 0.015738428511502027\n",
      "OOF Metric with postprocessing 0.055: 0.015737713723114766\n",
      "OOF Metric with postprocessing 0.06: 0.01573712002129769\n",
      "OOF Metric with postprocessing 0.065: 0.01573663848825139\n",
      "OOF Metric with postprocessing 0.07: 0.015736261806283187\n",
      "OOF Metric with postprocessing 0.075: 0.0157359838873396\n",
      "OOF Metric with postprocessing 0.08: 0.015735799607277563\n",
      "OOF Metric with postprocessing 0.085: 0.015735704610627017\n",
      "OOF Metric with postprocessing 0.09: 0.01573569516413499\n",
      "OOF Metric with postprocessing 0.095: 0.015735768044893565\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "\n",
    "\n",
    "f = 0.01\n",
    "factor = np.arange(0, 0.1, 0.005)\n",
    "\n",
    "for f in factor:\n",
    "    res_ = res.copy()\n",
    "    for col in res.columns:\n",
    "        res_[col] *= (1 - f)\n",
    "#         res_[col] += (f / len(res_[col]))\n",
    "\n",
    "\n",
    "#     res_.loc[train_features['cp_type'] == 0, train_targets.columns] = 0\n",
    "\n",
    "    for _target in train_targets.columns:\n",
    "        metrics.append(log_loss(train_targets.loc[:, _target], res_.loc[:, _target]))\n",
    "\n",
    "    print(f'OOF Metric with postprocessing {f}: {np.mean(metrics)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "# for col in res.columns:\n",
    "#     res[col] *= 0.986\n",
    "#     res.loc[res[col] >= 0.030184] = 0.03\n",
    "#     res.loc[res[col] <= 0.000411] = 0.0008\n",
    "\n",
    "res.loc[train_features['cp_type'] == 0, train_targets.columns] = 0\n",
    "\n",
    "for _target in train_targets.columns:\n",
    "    metrics.append(log_loss(train_targets.loc[:, _target], res.loc[:, _target]))\n",
    "    \n",
    "print(f'OOF Metric with postprocessing: {np.mean(metrics)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in res.columns:\n",
    "        ss[col] *= (1 - f)\n",
    "        ss[col] += (f / len(res[col]))\n",
    "        \n",
    "ss.loc[test_features['cp_type'] == 0, train_targets.columns] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('submission', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
